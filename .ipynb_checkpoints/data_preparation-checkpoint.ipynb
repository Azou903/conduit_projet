{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aefa80bb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f7ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474bf7f",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "713b68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"data/processed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a8398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "train_x = list(train[\"processed_tweet\"])\n",
    "test_x = list(test[\"processed_tweet\"])\n",
    "train_y = list(train[\"label\"])\n",
    "test_y = list(test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c322851",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 6000\n",
    "vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "features_train = vectorizer.fit_transform(train_x).toarray()\n",
    "features_test = vectorizer.transform(test_x).toarray()\n",
    "vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9abbf01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features_train\n",
    "test_x = features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd30e58",
   "metadata": {},
   "source": [
    "Create the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d9a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_val = int(len(train_x)*0.2)\n",
    "\n",
    "val_x = pd.DataFrame(train_x[:len_val])\n",
    "train_x = pd.DataFrame(train_x[len_val:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:len_val])\n",
    "train_y = pd.DataFrame(train_y[len_val:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efdc0d",
   "metadata": {},
   "source": [
    "A first directory will be created for data prepared. Several will be done with different balancing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f18a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = input(\"What version of the prepared data is it?:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fbef062",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"data_prepared_{version}\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e7ad611",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_x).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "pd.DataFrame(test_y).to_csv(os.path.join(data_dir, 'test_y.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([val_y, val_x], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([train_y, train_x], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8efa7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = train_X = val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed3817",
   "metadata": {},
   "source": [
    "Upload the data to S3 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "580f71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = f'twitter_sentiment_{version}'\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "\n",
    "s3_folder = {f\"model_{version}\":{\"test\":test_location,\"val\":val_location,\"train\":train_location}}\n",
    "\n",
    "import json\n",
    "with open(\"data/s3_folders.json\", \"a+\") as f:\n",
    "    json.dump(s3_folder,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
