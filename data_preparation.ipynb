{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6735f598",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bb6af",
   "metadata": {},
   "source": [
    "We import all libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73f2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle, resample\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82e136",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c12b5b",
   "metadata": {},
   "source": [
    "For the model to be used, a vectorization of the tweets will be done. This because the argorithm to be used does not work with words but with matrices.\n",
    "A Countvectorizer will be performed. For this to be, the data will be split into train and testing. Right after, the vectorizer will be performed using the training set, for then transform the test set using the dictionary created by the training set. \n",
    "For the Vectorizer to be performed, the train and test data set need to be lists of words. \n",
    "\n",
    "First the data that has already been cleaned in the exploration step is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd516e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"data/processed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4edcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle(data,random_state =1) #data will be randomly shuffled so then when resampling the data it can be "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df43de2",
   "metadata": {},
   "source": [
    "### 1. Not balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1724ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"unbalanced\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a248d7",
   "metadata": {},
   "source": [
    "Splitting the data into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cab558",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state = 1) #control the random sampling to get reproductible results. \n",
    "#We will be using the same distribution of data so we can control for the balancing techniques not to be biased by using different data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c44ab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for testing dataset:\n",
      " 0    5955\n",
      "1     438\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "Labels for training dataset:\n",
      " 0    23765\n",
      "1     1804\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels for testing dataset:\\n\",test.label.value_counts(),\"\\n\")\n",
    "print(\"Labels for training dataset:\\n\",train.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf32d2",
   "metadata": {},
   "source": [
    "Now the datasets will be transformed into lists so they can be used as parameters for the vectorizer. \n",
    "\n",
    "The labels are separated from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e87b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list(train[\"processed_tweet\"])\n",
    "test_x = list(test[\"processed_tweet\"])\n",
    "\n",
    "train_y = list(train[\"label\"])\n",
    "test_y = list(test[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df758cbc",
   "metadata": {},
   "source": [
    "Now a vectorizer will be fit using a dictionary of 5.000 words. It will be fit using the training data set and then will be used to transform the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "773d2883",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "features_train = vectorizer.fit_transform(train_x).toarray()\n",
    "features_test = vectorizer.transform(test_x).toarray()\n",
    "vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654eebe",
   "metadata": {},
   "source": [
    "We will save the vocabulary in a json file so it can ba accessed later to process new tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced29e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for i in vocabulary:\n",
    "    vocabulary[i] = vocabulary[i].item() #the value in each pair is a numpy object, we use .item() to transform it into native object.\n",
    "\n",
    "with open(f\"data/vocabulary_{version}.json\", \"w\") as file:\n",
    "    json.dump(vocabulary,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3c4d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features_train #Now the train and test dataframes X are not processed tweets by arrays of numbers. \n",
    "test_x = features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec179d5",
   "metadata": {},
   "source": [
    "A validation set will be created out of the training set so it can be passed to the algorithm in sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95bb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_val = int(len(train_x)*0.2) #the length of the validation dataset will be 20% of that of the whole training set.\n",
    "\n",
    "val_x = pd.DataFrame(train_x[:len_val])\n",
    "train_x = pd.DataFrame(train_x[len_val:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:len_val])\n",
    "train_y = pd.DataFrame(train_y[len_val:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05854a3f",
   "metadata": {},
   "source": [
    "A first directory will be created for data prepared. Several will be done with different balancing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5097fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"data_prepared_{version}\" #a new folder will be created keeping in mind the version of the different balancing of the data.\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e091c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all data frames will be saved in the newly created folder.\n",
    "\n",
    "pd.DataFrame(test_x).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False) \n",
    "pd.DataFrame(test_y).to_csv(os.path.join(data_dir, 'test_y.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([val_y, val_x], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([train_y, train_x], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64c6c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = train_X = val_X = train_y = val_y = None \n",
    "#given the amount of resources the dataframes take, it is a good idea to delete them from the Ram after they have been permanently saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca4833",
   "metadata": {},
   "source": [
    "Finally, the training and validation as well as the test (the X part) will be ppload the data to S3 storage. From there they are to be called to be used in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9444792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker #The sagemaker module will be used to upload the data to s3.\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = f'twitter_sentiment_{version}'\n",
    "\n",
    "#the location of the information inside of S3 is saved so it can be referenced later. It is saved as string at the same time as is uploaded.\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix) \n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "\n",
    "#The location for all the data is permanently saved as a file inside the folder so it can be accessed by other modules.\n",
    "locations = \"%s = {'test': '%s','train': '%s','val':'%s'}\" % (version,test_location,train_location,val_location) #creating a text for a definition of a variable in python\n",
    "\n",
    "with open(\"data/s3_folders.py\", \"w\") as file:\n",
    "    file.write(locations)\n",
    "    file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d73b3",
   "metadata": {},
   "source": [
    "### 2. Undersampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c224107",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"underSample\" #this version is the undersampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "377f1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state = 1) #control the random sampling to get reproductible results. \n",
    "#We will be using the same distribution of data so we can control for the balancing techniques not to be biased by using different data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4c77fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for testing dataset:\n",
      " 0    5955\n",
      "1     438\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "Labels for training dataset:\n",
      " 0    23765\n",
      "1     1804\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels for testing dataset:\\n\",test.label.value_counts(),\"\\n\")\n",
    "print(\"Labels for training dataset:\\n\",train.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11def414",
   "metadata": {},
   "source": [
    "Now, for the balancing, we start by dividing the training data into the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bf77cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = train[train.label==0]\n",
    "violent = train[train.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd572360",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_underSampled = resample(normal, #we will resample the normal tweets\n",
    "                              replace = False, #we set replacement to false, since we are downsizing the data, is better to avoid repeating the observations\n",
    "                               n_samples = len(violent), #we match the new sample to the length of the violent sample\n",
    "                               random_state = 1) # we keep reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdd04171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New labels for training dataset:\n",
      " 1    1804\n",
      "0    1804\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([violent, normal_underSampled])\n",
    "print(\"New labels for training dataset:\\n\",train.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6c2db7",
   "metadata": {},
   "source": [
    "Now that the datasets are balanced, they will be transformed into lists so they can be used as parameters for the vectorizer. \n",
    "\n",
    "The labels are separated from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c58cb7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list(train[\"processed_tweet\"])\n",
    "test_x = list(test[\"processed_tweet\"])\n",
    "\n",
    "train_y = list(train[\"label\"])\n",
    "test_y = list(test[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ecc0d9",
   "metadata": {},
   "source": [
    "Now a vectorizer will be fit using a dictionary of 5.000 words. It will be fit using the training data set and then will be used to transform the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fa71d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "features_train = vectorizer.fit_transform(train_x).toarray()\n",
    "features_test = vectorizer.transform(test_x).toarray()\n",
    "vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada164e2",
   "metadata": {},
   "source": [
    "We will save the vocabulary in a json file so it can ba accessed later to process new tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf572c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for i in vocabulary:\n",
    "    vocabulary[i] = vocabulary[i].item() #the value in each pair is a numpy object, we use .item() to transform it into native object.\n",
    "\n",
    "with open(f\"data/vocabulary_{version}.json\", \"w\") as file:\n",
    "    json.dump(vocabulary,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e1a47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features_train #Now the train and test dataframes X are not processed tweets by arrays of numbers. \n",
    "test_x = features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b612ee",
   "metadata": {},
   "source": [
    "A validation set will be created out of the training set so it can be passed to the algorithm in sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c9834a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_val = int(len(train_x)*0.2) #the length of the validation dataset will be 20% of that of the whole training set.\n",
    "\n",
    "val_x = pd.DataFrame(train_x[:len_val])\n",
    "train_x = pd.DataFrame(train_x[len_val:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:len_val])\n",
    "train_y = pd.DataFrame(train_y[len_val:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a99bf",
   "metadata": {},
   "source": [
    "A first directory will be created for data prepared. Several will be done with different balancing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f4fc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"data_prepared_{version}\" #a new folder will be created keeping in mind the version of the different balancing of the data.\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6e9b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all data frames will be saved in the newly created folder.\n",
    "\n",
    "pd.DataFrame(test_x).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False) \n",
    "pd.DataFrame(test_y).to_csv(os.path.join(data_dir, 'test_y.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([val_y, val_x], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([train_y, train_x], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "916d9bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = train_X = val_X = train_y = val_y = None \n",
    "#given the amount of resources the dataframes take, it is a good idea to delete them from the Ram after they have been permanently saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e3dce",
   "metadata": {},
   "source": [
    "Finally, the training and validation as well as the test (the X part) will be ppload the data to S3 storage. From there they are to be called to be used in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b790b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker #The sagemaker module will be used to upload the data to s3.\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = f'twitter_sentiment_{version}'\n",
    "\n",
    "#the location of the information inside of S3 is saved so it can be referenced later. It is saved as string at the same time as is uploaded.\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix) \n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "\n",
    "\n",
    "locations = \"%s = {'test': '%s','train': '%s','val':'%s'}\" % (version,test_location,train_location,val_location) #creating a text for a definition of a variable in python\n",
    "\n",
    "with open(\"data/s3_folders.py\", \"a+\") as file:\n",
    "    file.write(locations)\n",
    "    file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca1c74",
   "metadata": {},
   "source": [
    "### 3. OverSampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3952949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"overSample\" #this version is the oversampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9859d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state = 1) #control the random sampling to get reproductible results. \n",
    "#We will be using the same distribution of data so we can control for the balancing techniques not to be biased by using different data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcaf185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for testing dataset:\n",
      " 0    5955\n",
      "1     438\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "Labels for training dataset:\n",
      " 0    23765\n",
      "1     1804\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels for testing dataset:\\n\",test.label.value_counts(),\"\\n\")\n",
    "print(\"Labels for training dataset:\\n\",train.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82af02",
   "metadata": {},
   "source": [
    "Now, for the balancing, we start by dividing the training data into the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "409c36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = train[train.label==0]\n",
    "violent = train[train.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e57e6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "violent_overSampled = resample(violent, #we will resample the violent tweets\n",
    "                              replace = True, #we set replacement to False, since we need to resample several times the same observations\n",
    "                               n_samples = len(normal), #we match the new sample to the length of the normal sample\n",
    "                               random_state = 1) # we keep reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17ee5cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New labels for training dataset:\n",
      " 1    23765\n",
      "0    23765\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([normal, violent_overSampled])\n",
    "print(\"New labels for training dataset:\\n\",train.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbdf54f",
   "metadata": {},
   "source": [
    "Now that the datasets are balanced, they will be transformed into lists so they can be used as parameters for the vectorizer. \n",
    "\n",
    "The labels are separated from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72d46159",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list(train[\"processed_tweet\"])\n",
    "test_x = list(test[\"processed_tweet\"])\n",
    "\n",
    "train_y = list(train[\"label\"])\n",
    "test_y = list(test[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78677c23",
   "metadata": {},
   "source": [
    "Now a vectorizer will be fit using a dictionary of 5.000 words. It will be fit using the training data set and then will be used to transform the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fef4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "features_train = vectorizer.fit_transform(train_x).toarray()\n",
    "features_test = vectorizer.transform(test_x).toarray()\n",
    "vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d03582",
   "metadata": {},
   "source": [
    "We will save the vocabulary in a json file so it can ba accessed later to process new tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd0ed93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for i in vocabulary:\n",
    "    vocabulary[i] = vocabulary[i].item() #the value in each pair is a numpy object, we use .item() to transform it into native object.\n",
    "\n",
    "with open(f\"data/vocabulary_{version}.json\", \"w\") as file:\n",
    "    json.dump(vocabulary,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8708ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features_train #Now the train and test dataframes X are not processed tweets by arrays of numbers. \n",
    "test_x = features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd310fb",
   "metadata": {},
   "source": [
    "A validation set will be created out of the training set so it can be passed to the algorithm in sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f20cb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_val = int(len(train_x)*0.2) #the length of the validation dataset will be 20% of that of the whole training set.\n",
    "\n",
    "val_x = pd.DataFrame(train_x[:len_val])\n",
    "train_x = pd.DataFrame(train_x[len_val:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:len_val])\n",
    "train_y = pd.DataFrame(train_y[len_val:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ae2ea",
   "metadata": {},
   "source": [
    "A first directory will be created for data prepared. Several will be done with different balancing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93ac9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"data_prepared_{version}\" #a new folder will be created keeping in mind the version of the different balancing of the data.\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f2de34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all data frames will be saved in the newly created folder.\n",
    "\n",
    "pd.DataFrame(test_x).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False) \n",
    "pd.DataFrame(test_y).to_csv(os.path.join(data_dir, 'test_y.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([val_y, val_x], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43e0d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_y, train_x], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aff91a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = train_X = val_X = train_y = val_y = None \n",
    "#given the amount of resources the dataframes take, it is a good idea to delete them from the Ram after they have been permanently saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a04932",
   "metadata": {},
   "source": [
    "Finally, the training and validation as well as the test (the X part) will be ppload the data to S3 storage. From there they are to be called to be used in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0feeb0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker #The sagemaker module will be used to upload the data to s3.\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = f'twitter_sentiment_{version}'\n",
    "\n",
    "#the location of the information inside of S3 is saved so it can be referenced later. It is saved as string at the same time as is uploaded.\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix) \n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "\n",
    "\n",
    "locations = \"%s = {'test': '%s','train': '%s','val':'%s'}\" % (version,test_location,train_location,val_location) #creating a text for a definition of a variable in python\n",
    "\n",
    "with open(\"data/s3_folders.py\", \"a+\") as file:\n",
    "    file.write(locations)\n",
    "    file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc91a7",
   "metadata": {},
   "source": [
    "### 4. Combined technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd28e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"combined\" #this version is the oversampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eae45910",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state = 1) #control the random sampling to get reproductible results. \n",
    "#We will be using the same distribution of data so we can control for the balancing techniques not to be biased by using different data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05ed1438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for testing dataset:\n",
      " 0    5955\n",
      "1     438\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "Labels for training dataset:\n",
      " 0    23765\n",
      "1     1804\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels for testing dataset:\\n\",test.label.value_counts(),\"\\n\")\n",
    "print(\"Labels for training dataset:\\n\",train.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18aec88",
   "metadata": {},
   "source": [
    "Now, for the balancing, we start by dividing the training data into the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c90e9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = train[train.label==0]\n",
    "violent = train[train.label==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5aa753",
   "metadata": {},
   "source": [
    "For the \"combined\" approach we \"split the difference\" between the size of the samples. For this we find the size from getting the difference between the samples, then adding half the difference to the length of the \"violent\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc5f1251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12784"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = int(len(violent) + ((len(normal)-len(violent))/2))\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ef52a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "violent_overSampled = resample(violent, #we will resample the violent tweets\n",
    "                              replace = True, #we set replacement to False, since we need to resample several times the same observations\n",
    "                               n_samples = sample_size, #we match both samples\n",
    "                               random_state = 1) # we keep reproducible results\n",
    "\n",
    "normal_underSampled = resample(normal, #we will resample the normal tweets\n",
    "                              replace = False, #we set replacement to false, since we are downsizing the data, is better to avoid repeating the observations\n",
    "                               n_samples = sample_size, #we match both samples\n",
    "                               random_state = 1) # we keep reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "676e3959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New labels for training dataset:\n",
      " 1    12784\n",
      "0    12784\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([normal_underSampled, violent_overSampled])\n",
    "print(\"New labels for training dataset:\\n\",train.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f03cf8",
   "metadata": {},
   "source": [
    "Now that the datasets are balanced, they will be transformed into lists so they can be used as parameters for the vectorizer. \n",
    "\n",
    "The labels are separated from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9aefe329",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list(train[\"processed_tweet\"])\n",
    "test_x = list(test[\"processed_tweet\"])\n",
    "\n",
    "train_y = list(train[\"label\"])\n",
    "test_y = list(test[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b076ac",
   "metadata": {},
   "source": [
    "Now a vectorizer will be fit using a dictionary of 5.000 words. It will be fit using the training data set and then will be used to transform the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7179028",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "features_train = vectorizer.fit_transform(train_x).toarray()\n",
    "features_test = vectorizer.transform(test_x).toarray()\n",
    "vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee5040",
   "metadata": {},
   "source": [
    "We will save the vocabulary in a json file so it can ba accessed later to process new tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f626f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for i in vocabulary:\n",
    "    vocabulary[i] = vocabulary[i].item() #the value in each pair is a numpy object, we use .item() to transform it into native object.\n",
    "\n",
    "with open(f\"data/vocabulary_{version}.json\", \"w\") as file:\n",
    "    json.dump(vocabulary,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79d59502",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features_train #Now the train and test dataframes X are not processed tweets by arrays of numbers. \n",
    "test_x = features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63146506",
   "metadata": {},
   "source": [
    "A validation set will be created out of the training set so it can be passed to the algorithm in sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5869565",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_val = int(len(train_x)*0.2) #the length of the validation dataset will be 20% of that of the whole training set.\n",
    "\n",
    "val_x = pd.DataFrame(train_x[:len_val])\n",
    "train_x = pd.DataFrame(train_x[len_val:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:len_val])\n",
    "train_y = pd.DataFrame(train_y[len_val:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d11cd9",
   "metadata": {},
   "source": [
    "A first directory will be created for data prepared. Several will be done with different balancing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22d8449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"data_prepared_{version}\" #a new folder will be created keeping in mind the version of the different balancing of the data.\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f358a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all data frames will be saved in the newly created folder.\n",
    "\n",
    "pd.DataFrame(test_x).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False) \n",
    "pd.DataFrame(test_y).to_csv(os.path.join(data_dir, 'test_y.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([val_y, val_x], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7036c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_y, train_x], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f01457ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = train_X = val_X = train_y = val_y = None \n",
    "#given the amount of resources the dataframes take, it is a good idea to delete them from the Ram after they have been permanently saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2af0aa",
   "metadata": {},
   "source": [
    "Finally, the training and validation as well as the test (the X part) will be ppload the data to S3 storage. From there they are to be called to be used in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "75b179bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker #The sagemaker module will be used to upload the data to s3.\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = f'twitter_sentiment_{version}'\n",
    "\n",
    "#the location of the information inside of S3 is saved so it can be referenced later. It is saved as string at the same time as is uploaded.\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix) \n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "\n",
    "\n",
    "locations = \"%s = {'test': '%s','train': '%s','val':'%s'}\" % (version,test_location,train_location,val_location) #creating a text for a definition of a variable in python\n",
    "\n",
    "with open(\"data/s3_folders.py\", \"a+\") as file:\n",
    "    file.write(locations)\n",
    "    file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
