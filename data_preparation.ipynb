{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b931749",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db189000",
   "metadata": {},
   "source": [
    "We import all libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bc74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle, resample\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4540279",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487ec1f",
   "metadata": {},
   "source": [
    "For the model to be used, a vectorization of the tweets will be done. This because the argorithm to be used does not work with words but with matrices.\n",
    "A Countvectorizer will be performed. For this to be, the data will be split into train and testing. Right after, the vectorizer will be performed using the training set, for then transform the test set using the dictionary created by the training set. \n",
    "For the Vectorizer to be performed, the train and test data set need to be lists of words. \n",
    "\n",
    "First the data that has already been cleaned in the exploration step is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a381ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"data/processed_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54e198",
   "metadata": {},
   "source": [
    "### 1. Not balancing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf177e",
   "metadata": {},
   "source": [
    "Splitting the data into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2db45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state = 1) #control the random sampling to get reproductible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5598ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for testing dataset:\n",
      " 0    5951\n",
      "1     442\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "Labels for training dataset:\n",
      " 0    23769\n",
      "1     1800\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels for testing dataset:\\n\",test.label.value_counts(),\"\\n\")\n",
    "print(\"Labels for training dataset:\\n\",train.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85c6dc3",
   "metadata": {},
   "source": [
    "Now the datasets will be transformed into lists so they can be used as parameters for the vectorizer. \n",
    "\n",
    "The labels are separated from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce7e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list(train[\"processed_tweet\"])\n",
    "test_x = list(test[\"processed_tweet\"])\n",
    "\n",
    "train_y = list(train[\"label\"])\n",
    "test_y = list(test[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb48b78",
   "metadata": {},
   "source": [
    "Now a vectorizer will be fit using a dictionary of 6.000 words. It will be fit using the training data set and then will be used to transform the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c5b587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 6000\n",
    "vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "features_train = vectorizer.fit_transform(train_x).toarray()\n",
    "features_test = vectorizer.transform(test_x).toarray()\n",
    "vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea541f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features_train #Now the train and test dataframes X are not processed tweets by arrays of numbers. \n",
    "test_x = features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e69ff7",
   "metadata": {},
   "source": [
    "A validation set will be created out of the training set so it can be passed to the algorithm in sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "813fcea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_val = int(len(train_x)*0.2) #the length of the validation dataset will be 20% of that of the whole training set.\n",
    "\n",
    "val_x = pd.DataFrame(train_x[:len_val])\n",
    "train_x = pd.DataFrame(train_x[len_val:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:len_val])\n",
    "train_y = pd.DataFrame(train_y[len_val:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e3a3c0",
   "metadata": {},
   "source": [
    "A first directory will be created for data prepared. Several will be done with different balancing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = input(\"What version of the prepared data is it?:\") #this version variable will be set several times for all versions of the balancing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff48fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"data_prepared_{version}\" #a new folder will be created keeping in mind the version of the different balancing of the data.\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aff75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all data frames will be saved in the newly created folder.\n",
    "\n",
    "pd.DataFrame(test_x).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False) \n",
    "pd.DataFrame(test_y).to_csv(os.path.join(data_dir, 'test_y.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([val_y, val_x], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([train_y, train_x], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08682125",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = train_X = val_X = train_y = val_y = None \n",
    "#given the amount of resources the dataframes take, it is a good idea to delete them from the Ram after they have been permanently saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776995c2",
   "metadata": {},
   "source": [
    "Finally, the training and validation as well as the test (the X part) will be ppload the data to S3 storage. From there they are to be called to be used in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b63c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker #The sagemaker module will be used to upload the data to s3.\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = f'twitter_sentiment_{version}'\n",
    "\n",
    "#the location of the information inside of S3 is saved so it can be referenced later. It is saved as string at the same time as is uploaded.\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix) \n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "\n",
    "#The location for all the data is permanently saved as a json file inside the folder so it can be accessed by other modules.\n",
    "s3_folder = {f\"model_{version}\":{\"test\":test_location,\"val\":val_location,\"train\":train_location}}\n",
    "\n",
    "import json\n",
    "with open(\"data/s3_folders.json\", \"a+\") as f:\n",
    "    json.dump(s3_folder,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71edf1bd",
   "metadata": {},
   "source": [
    "### 2. Undersampling technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
